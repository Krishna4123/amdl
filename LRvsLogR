

1. Linear Regression â€” Deep Insight

What is Linear Regression?

Linear Regression models the **relationship between a continuous dependent variable ( y ) and one or more independent variables ( x ) by fitting a linear equation.

Model Equation

[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \varepsilon
]

* ( \beta_0 ): Intercept
* ( \beta_i ): Coefficients (weights)
* ( \varepsilon ): Noise / error term

---

 ğŸ§  Intuition

Linear regression finds a straight line (or hyperplane) that best fits the data such that the **sum of squared errors** between predicted and actual values is minimized.

> â€œBest fitâ€ = minimum total error

---

 ğŸ§® Mathematical Foundation

Loss Function â€” Mean Squared Error (MSE)

[
J(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
]

Why squared?

* Penalizes large errors more
* Differentiable (easy optimization)

---

 Optimization Techniques

 1ï¸âƒ£ Normal Equation (Closed Form)

[
\beta = (X^T X)^{-1} X^T y
]

 Exact solution
Computationally expensive for large datasets

---

 2ï¸âƒ£ Gradient Descent

Iteratively updates parameters:
[
\beta_j := \beta_j - \alpha \frac{\partial J}{\partial \beta_j}
]

âœ” Scales well
âœ” Used in deep learning
âŒ Needs tuning (learning rate)

---

 ğŸ“ Assumptions of Linear Regression

1. Linearity â€“ relationship is linear
2. Independence â€“ observations are independent
3. Homoscedasticity â€“ constant variance of errors
4. Normality â€“ residuals are normally distributed
5. No multicollinearity â€“ predictors not highly correlated

Violation â†’ unreliable predictions

---

 ğŸ“Š Evaluation Metrics

* MSE / RMSE
* **MAE**
* **RÂ² Score**
  [
  R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
  ]

---

âš  Limitations

* Cannot model non-linear relationships
* Sensitive to outliers
* Assumption-heavy

---

ğŸ§ª Variants

| Variant          | Purpose                  |
| ---------------- | ------------------------ |
| Ridge Regression | Reduces overfitting (L2) |
| Lasso Regression | Feature selection (L1)   |
| ElasticNet       | L1 + L2                  |

---

---

# ğŸ”· 2. Logistic Regression â€” Deep Insight

ğŸ“Œ What is Logistic Regression?

Logistic Regression is a **classification algorithm**, used when the target variable is **binary** (0 or 1).

Despite its name, it is a **classification model**, not regression.

---

ğŸ§  Intuition

Instead of predicting a continuous value, logistic regression predicts the **probability** that a data point belongs to a class.

It uses a **sigmoid function** to squash values between 0 and 1.

---

 ğŸ§® Mathematical Model

 Linear Combination

[
z = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
]

 Sigmoid Function

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

Output â†’ probability
[
P(y=1|x)
]

---

 ğŸ“‰ Decision Boundary

Classify as:

* 1 if ( P \ge 0.5 )
* 0 otherwise

This creates a **linear decision boundary**.

---

ğŸ“ Loss Function â€” Log Loss (Cross-Entropy)

[
J(\beta) = -\frac{1}{n} \sum \left[ y\log(\hat{y}) + (1-y)\log(1-\hat{y}) \right]
]

Why not MSE?

* MSE leads to slow convergence
* Log loss aligns with probability theory

---

 âš™ Optimization

Uses Gradient Descent or advanced optimizers:

* Newton-Raphson
* LBFGS
* SGD

---

 ğŸ“Š Evaluation Metrics

* Accuracy
* Precision, Recall
* F1-score
* ROCâ€“AUC
* Confusion Matrix

---

 ğŸ“ Assumptions

1. Binary outcome
2. Independent observations
3. Little or no multicollinearity
4. Linear relationship between log-odds and features

---

 âš  Limitations

* Cannot handle complex non-linear patterns
* Sensitive to outliers
* Needs balanced data

---

 ğŸ§ª Extensions

| Type                 | Description     |
| -------------------- | --------------- |
| Multinomial Logistic | Multi-class     |
| Ordinal Logistic     | Ordered classes |
| Regularized Logistic | L1 / L2         |

---

---

# ğŸ” Linear vs Logistic Regression (Comparison)

| Feature           | Linear Regression | Logistic Regression |
| ----------------- | ----------------- | ------------------- |
| Output            | Continuous        | Probability         |
| Task              | Regression        | Classification      |
| Loss              | MSE               | Log Loss            |
| Activation        | None              | Sigmoid             |
| Decision Boundary | N/A               | Linear              |
| Output Range      | (-âˆ, +âˆ)          | (0, 1)              |

---

ğŸ§  Real-World Use Cases

* Linear: House price prediction, salary estimation
* Logistic: Spam detection, disease diagnosis, fraud detection

